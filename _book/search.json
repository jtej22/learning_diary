[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josiah’s Remote Sensing Learning Diary",
    "section": "",
    "text": "1 Self-Introduction\nWelcome to my learning diary on remote sensing! I am currently a MSc Urban Spatial Science student at UCL Centre for Advanced Spatial Analytics (CASA). Prior to this, I graduated from UCL with a BSc in Geography with Economics. I am mainly interested in using spatial analysis techniques to urban problems from a policymaking perspective, which explains why I am at CASA now. After graduation, I will be working with Housing Development Board (HDB) in Singapore, the key agency responsible for public housing in the country. I have also interned with Singapore’s Ministry of Sustainability and Environment before, and am interested in urban environmental issues. Before my undergraduate studies, I also had a brief stint in the Singapore Armed Forces Mapping Unit where I had some exposure to pre-processing satellite imagery and digitizing maps. Through this module, I hope to learn necessary and relevant skills and techniques that can be applied to both urban housing and environmental contexts."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "See Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Boone, R. B., Thirgood, S. J. and Hopcraft, J. G. C. (2006)\n‘Serengeti wildebeest migratory patterns modeled from rainfall and\nnew vegetation growth’, Ecology, 87(8), pp. 1987–1994.\nAvailable at: https://www.jstor.org/stable/20069184.\n\n\nBuiten, H. J. and Putten, B. van (1997) ‘Quality assessment of\nremote sensing image registration  analysis and testing of\ncontrol point residuals’, ISPRS Journal of Photogrammetry and\nRemote Sensing, 52(2), pp. 57–73. doi: 10.1016/S0924-2716(97)83001-8.\n\n\nCorcione, V. et al. (2021) ‘A Sensitivity Analysis on the\nSpectral Signatures of Low-Backscattering Sea Areas in Sentinel-1 SAR\nImages’, Remote Sensing, 13(6), p. 1183. doi: 10.3390/rs13061183.\n\n\nDegerickx, J., Hermy, M. and Somers, B. (2020) ‘Mapping Functional\nUrban Green Types Using High Resolution Remote Sensing Data’,\nSustainability, 12(5), p. 2144. doi: 10.3390/su12052144.\n\n\nFaridatul, M. I. and Ahmed, B. (2020) ‘Assessing Agricultural\nVulnerability to Drought in a Heterogeneous Environment: A Remote\nSensing-Based Approach’, Remote Sensing, 12(20), p.\n3363. doi: 10.3390/rs12203363.\n\n\nFarwell, L. S. et al. (2021) ‘Satellite image texture\ncaptures vegetation heterogeneity and explains patterns of bird\nrichness’, Remote Sensing of Environment, 253, p.\n112175. doi: 10.1016/j.rse.2020.112175.\n\n\nFensholt, R. and Proud, S. R. (2012) ‘Evaluation of Earth\nObservation based global long term vegetation trends \nComparing GIMMS and MODIS global NDVI time series’, Remote\nSensing of Environment, 119, pp. 131–147. doi: 10.1016/j.rse.2011.12.015.\n\n\nHall-Beyer, M. (2017) ‘GLCM Texture: A Tutorial v. 3.0 March\n2017’.\n\n\nHaralick, R. M., Shanmugam, K. and Dinstein, I. (1973) ‘Textural\nfeatures for image classification’, IEEE Transactions on\nSystems, Man, and Cybernetics, SMC-3(6), pp. 610–621. doi: 10.1109/TSMC.1973.4309314.\n\n\nJensen, J. R. (2015) Introductory Digital Image Processing: A Remote\nSensing Perspective. Pearson Education, Incorporated.\n\n\nJuergens, C. and Meyer-Heß, M. F. (2022) ‘Experimental Analysis of\nGeo-spatial Data to Evaluate Urban Greenspace: A Case Study in Dortmund,\nGermany’, KN - Journal of Cartography and Geographic\nInformation, 72(2), pp. 153–171. doi: 10.1007/s42489-022-00107-5.\n\n\nKlonus, S. and Ehlers, M. (2009) ‘2009 12th international\nconference on information fusion’, in, pp. 1409–1416.\n\n\nLi, D. et al. (2015) ‘Object-Based Urban Tree Species\nClassification Using Bi-Temporal WorldView-2 and WorldView-3\nImages’, Remote Sensing, 7(12), pp. 16917–16937. doi: 10.3390/rs71215861.\n\n\nNeyns, R. and Canters, F. (2022) ‘Mapping of Urban Vegetation with\nHigh-Resolution Remote Sensing: A Review’, Remote\nSensing, 14(4), p. 1031. doi: 10.3390/rs14041031.\n\n\nNovak, K. (1992) ‘Rectification of Digital Imagery’,\nPHOTOGRAMMETRIC ENGINEERING.\n\n\nPinto, D. G. et al. (2017) ‘Correlations between spectral\nand biophysical data obtained in canola canopy cultivated in the\nsubtropical region of Brazil’, Pesquisa Agropecuária\nBrasileira, 52(10), pp. 825–832. doi: 10.1590/s0100-204x2017001000001.\n\n\nPratt, W. (2013) Introduction to Digital Image Processing. Boca\nRaton: CRC Press. Available at: https://learning.oreilly.com/library/view/introduction-to-digital/9781482216691/.\n\n\nQi, S. et al. (2022) ‘Bamboo Forest Mapping in China\nUsing the Dense Landsat 8 Image Archive and Google Earth Engine’,\nRemote Sensing, 14(3), p. 762. doi: 10.3390/rs14030762.\n\n\nRen, Z. et al. (2017) ‘Spatiotemporal analyses of urban\nvegetation structural attributes using multitemporal Landsat TM data and\nfield measurements’, Annals of Forest Science, 74(3),\npp. 1–14. doi: 10.1007/s13595-017-0654-x.\n\n\nShafizadeh-Moghadam, H. et al. (2021) ‘Google earth\nengine for large-scale land use and land cover mapping: An object-based\nclassification approach using spectral, textural and topographical\nfactors’, GIScience & Remote Sensing, 58(6), pp.\n914–928. doi: 10.1080/15481603.2021.1947623.\n\n\nTassi, A. and Vizzari, M. (2020) ‘Object-Oriented LULC\nClassification in Google Earth Engine Combining SNIC, GLCM, and Machine\nLearning Algorithms’, Remote Sensing, 12(22), p. 3776.\ndoi: 10.3390/rs12223776.\n\n\nTucker, C. J. et al. (2005) ‘An extended AVHRR\n8-km NDVI dataset compatible with MODIS and SPOT vegetation\nNDVI data’, International Journal of Remote Sensing,\n26(20), pp. 4485–4498. doi: 10.1080/01431160500168686.\n\n\nWang, K., Wang, T. and Liu, X. (2019) ‘A Review: Individual Tree\nSpecies Classification Using Integrated Airborne LiDAR and Optical\nImagery with a Focus on the Urban Environment’, Forests,\n10(1), p. 1. doi: 10.3390/f10010001.\n\n\nWen, D. et al. (2017) ‘Semantic classification of urban\ntrees using very high resolution satellite imagery’, IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote\nSensing, 10(4), pp. 1413–1424. doi: 10.1109/JSTARS.2016.2645798.\n\n\nWolberg, G. (1990) Digital image warping | wiley. New York:\nJohn Wiley- IEEE Computer Society. Available at: https://www.wiley.com/en-us/Digital+Image+Warping-p-9780818689444.\n\n\nZhang, X., Feng, X. and Jiang, H. (2010) ‘Object-oriented method\nfor urban vegetation mapping using IKONOS imagery’,\nInternational Journal of Remote Sensing, 31(1), pp. 177–196.\ndoi: 10.1080/01431160902882603."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Week 2: Quarto and Xaringan",
    "section": "",
    "text": "https://jtej22.github.io/w2_presentation/presentation.html#1"
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "1  Week 1: Introduction to Remote Sensing",
    "section": "",
    "text": "{r, echo = FALSE, out.width=‘40%’} xaringanExtra::embed_xaringan( url = “link”, ratio = “16:9”)"
  },
  {
    "objectID": "week1.html#learning-objectives",
    "href": "week1.html#learning-objectives",
    "title": "2  Week 1: Introduction to Remote Sensing",
    "section": "2.1 Learning Objectives",
    "text": "2.1 Learning Objectives\nThe learning objectives of both the lecture and practical are:\n\nUnderstand the science behind remote sensors and how satellite imagery is obtained\nUnderstand how remotely sensed data work\nSource, load and articulate the differences between Landsat and Sentinel data\nUndertake basic raster image statistics and processing\nEvaluate the (dis)advantages of each type of software you have used\nPull out and statistically compare spectral signatures"
  },
  {
    "objectID": "week1.html#summary-of-key-concepts",
    "href": "week1.html#summary-of-key-concepts",
    "title": "2  Week 1: Introduction to Remote Sensing",
    "section": "2.2 Summary of key concepts",
    "text": "2.2 Summary of key concepts\n\n2.2.1 Types of sensors\nThere are 2 main types of sensors, passive and active sensors. The key difference is that passive sensors (e.g. cameras, satellite sensors) usually detect reflected energy from the Sun while active sensors (e.g. LiDAR, radar, X-Ray) actively emit electromagnetic (EM) waves and detected the reflected waves.\nThis difference will be useful as the usefulness of the data from different types of sensors will be affected. Passive sensors which rely on reflected energy are thus dependent on the Sun, meaning that the time and day the image is captured will affect the imagery’s usefulness, as well as if there’s presence of cloud cover as clouds will be captured too. This may render the images less useful and users have to use images from another recorded day. Active sensors are more able to bypass clouds and this means that their recorded data is usually not affected by cloud cover and more useful. This also ties into the scientific concept of EM waves, as active sensors record data from a wider range of the EM spectrum compared to passive sensors, hence it can “see” through clouds and other atmospheric conditions.\n\n\n2.2.2 Data resolutions\nThe 4 types of resolutions are:\n\nSpatial: Size of the raster grid per pixel\nSpectral: Number of bands it records data in\nTemporal: Frequency of revisiting the site\nRadiometric: Differences in light or reflectance"
  },
  {
    "objectID": "week1.html#practical",
    "href": "week1.html#practical",
    "title": "2  Week 1: Introduction to Remote Sensing",
    "section": "2.3 Practical",
    "text": "2.3 Practical\nThe practical gave me the opportunity to explore the differences between Sentinel and Landsat data, as well as go through the practical process of searching for and downloading the relevant datasets. While I am slightly clearer on the general process, I am still perplexed by the vast options available such as choosing platforms, product types, and what is the best practice for choosing relevant imagery for different purposes.\nWhen exploring the Sentinel data it was helpful to see the various bands and that solidified my understanding of spectral bands practically.\n\n\n\n\n\nFig. 1: Bands available in Sentinel data\n\n\n\n\n\n2.3.1 Colour composites\nColour composites are a way of manipulating the way rasters are visualised (not actually modifying the raster data) so that we can focus on different aspects. The RGB image allows us to see the raster data in a way that is intuitive. There are other composites (band combinations) that are useful such as:\n\nFalse colour (Color infrared) composite: Emphasizes vegetation health, with denser vegetation appearing red while urban areas appear white\n\nThis is because vegetation absorbs red\n\nAtmospheric penetration composite: does not use visible bands so as to not be affected by atmospheric particles\n\nVegetation appears blue, urban area appears white, gray cyan or purple.\n\nShortwave infrared composite: Used to illustrate vegetation in different shades of green (differing densities) and brown areas represent built up or bare soil.\nMore information can be found on gisgeography\n\n\n\n2.3.2 Scatterplot analysis\nWe were exposed to a really cool method of using scatterplots to statistically analyse images, and created a plot of Band 8 (y-axis) against Band 4 (x-axis).\nFor this practical, my city of choice was Cape Town, South Africa, which was the city Andy used in the practical instructions. I chose this city as I wanted to focus on familiarising with the layout and tools in SNAP and R and not worry about differences in inputs from Andy’s example.\nThe scatterplot I obtained is as follows:\n\n\n\n\n\nFig. 2: Tasseled Cap using Cape Town imagery\n\n\n\n\nThis scatterplot can be interpreted as follows:\n\n\n\n\n\nFig. 3: Interpretation of Tasseled Caps\n\n\n\n\nSource: Remote Sensing 4113\n\n\n2.3.3 Resampling and Masking\nResampling and Masking are essential processes when dealing with raster data.\nMasking is essentially clipping like what we saw in CASA0005, and this can be done with vector data (ESRI Shapefile .shp) in SNAP.\nResampling is the change of spatial resolution (either increasing or decreasing) of the raster dataset. Resampling calculates new pixel values from the original pixel values in the original image.\nThere are various resampling techniques:\n\nNearest Neighbour Resampling\n\nTakes the cell centre from the input raster to determine the closes cell centre of the output\nFastest method because of its simplicity\nIdeal for categorical, nominal and ordinal data as it does not alter values\nE.g. useful for landcover classification raster grid\n\nBilinear Interpolation\n\nCalculates values of a grid based on 4 nearby grids\nAssigns the output cell value by taking a weighted average\nUseful when working with continuous datasets that do not have distinct boundaries\nE.g. useful for noise distance rasters\n\nCubic Convolution Interpolation\n\nSimilar to bilinear interpolation\nUses 16 nearest cells instead\nLong processing time\nUsually used for continuous surfaces where much noise exists\n\nMajority Resampling\n\nSimilar to nearest neighbour algorithm\nInstead uses the most common values in a filter window\n\n\n\n\n2.3.4 Spectral Signatures\nComparing Landsat and Sentinel products, we only focus on overlapping bands which are B2-4.\nIn R, we use the shapefiles for each landcover type to extract pixel values from the raster data. This is done using the R package terra. As with CASA0005, it is important to check the CRS of our data. We use terra’s extract function to extract values from our raster, and we repeat this for each landcover type for each image type (Landsat and Sentinel).\nUsing these values, we plot spectral profiles and density plots to observe differences between landcover types.\n\n\n\n\n\nFig. 4a: Sentinel Spectral Reflectance\n\n\n\n\n\n\n\n\n\nFig. 4b: Landsat Spectral Reflectance\n\n\n\n\nComparing the two outputs, we firstly see that the Landsat plot only has 3 land-cover types as there was no high-urban POI in the Landsat file (due to differences in areas covered). The number of bands plotted is the number of bands that are available in the product. Ideally, we want each landcover type to have a distinct signature at each band so that we can use the raster products for landuse classification.\nFor both plots, there isn’t much of a distinct spectral signature any of the landcover types, and there is a fair amount of overlap between the landcover types in each band. This means that the raster products used are not the most ideal for landcover classification and more image processing might be needed. It should also be noted that while the Landsat plot looks less differentiated, the mean values on the y-axis are of different scales from the y-axis values for the Sentinel plot."
  },
  {
    "objectID": "week1.html#applications-of-spectral-signatures",
    "href": "week1.html#applications-of-spectral-signatures",
    "title": "2  Week 1: Introduction to Remote Sensing",
    "section": "2.4 Applications of Spectral Signatures",
    "text": "2.4 Applications of Spectral Signatures\nAn interesting application of spectral signatures in academic studies is the analysis of the effects of low-backscattering areas of anthropogenic and natural origin on the azimuth autocorrelation function (AACF) using VV-polarised SAR measurements (Corcione et al., 2021). This is of great interest for the marine pollution community to better differentiate between natural low-backscattering or human pollution."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "2  Week 1: Introduction to Remote Sensing",
    "section": "2.5 Reflection",
    "text": "2.5 Reflection\n\n2.5.1 Reflection on data resolutions\nSpatial and temporal resolution are concepts that are fairly straightforward to me, as I used to be a mapper with the Singapore Armed Forces, where both types of resolutions were practical considerations when we were digitising areas of interest as it affected how clearly we could observe features and when choosing image files to use.\nSpectral resolution was of particular interest to me, as it is the key area that is involved in remote sensing analysis. Objects appear as a certain colour on satellite imagery because that is the wavelength that is reflected, and in the raster data from remote sensors, we can observe values for each wavelength across the EM spectrum. This allows us to create a spectral signature which helps us identify different features or land-covers. This concept is explored more practically later.\nRadiometric resolution is the ability of a sensor to detect and record differences in energy, and the higher radiometric resolution a sensor has, the more sensitive it is to differences on the ground. This means the image is of better quality.\n\n\n2.5.2 POIs\nJust a note on POIs, while the practical suggested various land-cover types such as bare earth, water grass, forest and urban, I interpreted these land-cover types according to my prior experience of landcover classification in Singapore. This meant I did not find any suitable POIs for the forest category as it was mostly bare earth, grass and farmland in the tile. Upon clarification with Andy, the main takeaway is that POIs depend on the area chosen and research objectives, and what types of landcover are present. It is up to the researcher to define the classifications.\nWe can also rely on landcovers classified by other researchers such as Dynamic World and reference their 9 land use and cover types (e.g. trees, grass, shrub & scrub and snow & ice)\n\n\n2.5.3 Spectral Signatures\nSpectral signatures will be key in my remote sensing journey moving forward. Regarding resources for spectral signatures that might be useful in the future, there is the USGS Spectral Library where we can reference the spectral reflectance of various materials for identification purposes. There is also the Awesome Spectral Indices list which keeps track of classical and novel spectral indices for different Remote Sensing applications."
  },
  {
    "objectID": "week1.html#references",
    "href": "week1.html#references",
    "title": "1  Week 1: Introduction to Remote Sensing",
    "section": "1.6 References",
    "text": "1.6 References\n\n\n\n\n\n\n\nCorcione, V. et al. (2021) “A Sensitivity Analysis on the Spectral Signatures of Low-Backscattering Sea Areas in Sentinel-1 SAR Images,” Remote Sensing, 13(6), p. 1183. doi: 10.3390/rs13061183."
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "3  Week 3: Corrections",
    "section": "",
    "text": "Understand and apply types of corrections that can be applied to remotely sensed data\nUnderstand and apply methods of data joining and enhancement\nUnderstand how enhancements can be used to emphasize certain features or spectral traits"
  },
  {
    "objectID": "week3.html#summary-of-key-concepts",
    "href": "week3.html#summary-of-key-concepts",
    "title": "3  Week 3: Corrections",
    "section": "3.2 Summary of Key Concepts",
    "text": "3.2 Summary of Key Concepts\n\n3.2.1 Overview\nIn this lecture, we mainly looked at types of corrections that could be applied to remotely sensed data:\n\nGeometric\n\nUse Ground Control Points (GCPs) to match points in the new image to a reference dataset.\nIdeally done with backward mapping (output to input) so that we can get a value in the original input image for every value in the gold standard (Jensen, 2015, p. 247).\nAnd resample the final raster\n\nAtmospheric\n\nRelative (e.g. Dark Object Subtraction or Pseudo-Invariant Features)\nAbsolute; using atmospheric radiative transfer models, assuming atmospheric measurements are available (e.g. Py6S)\n\nOrthorectification/ Topographic Correction\n\nRemoving distortions such that the pixels viewed at nadir (straight down)\nRequires sensor geometry and DEM.\nCosine correction can be used (Jensen, 2015)\n\nRadiometric\n\nUsing Digital Number to obtain spectral radiance\n\n\nWe also looked at how remotely sensed data can be mosaicked and enhanced. Mosaicking is the process of joining 2 or more images together, similar to merging in the sense of polygons.\nTypes of Enhancements and methods:\n\nContrast enhancement:\n\nMinimum-Maximum\nPercentage Linear and Standard Deviation\nPiecewise Linear Contrast Stretch\n\nBand Ratioing\nFiltering\nPrincipal Component Analysis (PCA)\n\n\n\n3.2.2 Geometric Corrections and Mosaicking (Jensen, 2015)\nFor this week’s learning diary, I will be focusing on types of geometric corrections and the mosaicking process, based on Jensen’s textbook (2015).\nGround Control Points are locations on the road surface that can be easily and accurately identified on a map. Each GCP should have 2 distinct sets of coordinates, image and map coordinates.\nImage-to-map rectification\nThis is the process by which the geometry of an image is made planimetric, to remove distortion caused by topographic relief displacement.\nSteps:\n\nSpatial Interpolation: The geometric relationship between input pixel coordinates and reference map coordinates must be identified, to establish the nature of transformation to be applied to all other pixels.\n\nUnsystematic errors in the new image produced by changes in attitude (roll, pitch and yaw) or altitude\nA first-order, six-parameter, linear transformation is sufficient to rectify the imagery to a geographic frame of reference.\nThis type of transformation can model 6 kinds of distortions (Novak, 1992; Buiten and Putten, 1997)\n\nTranslation in x and y\nScale changes in x and y\nSkew\nRotation\n\nForward Mapping (input-to-output):\n\n\\[\nx = a_0 + a_1 x' + a_2 y'\n\\]\n\\[\ny = b_0 + b_1 x' + b_2 y'\n\\]\nThis forward mapping logic is useful when we rectify the location of discrete coordinates along a linear feature. However, when we are filling a rectified output grid with values from an unrectified input image, forward mapping logic is not very useful as the output location may not fall exactly on a x,y output map coordinate.\nThis can result in output matrix pixels with no output values (Wolberg, 1990)\n\nInverse Mapping (output-to-input)\n\n\\[\nx' = a_0 + a_1 x + a_2 y\n\\]\n\\[\ny' = b_0 + b_1 x + b_2 y\n\\]\nThe rectified output matrix is filled systematically, with the equation using the six coefficient to determine where to get a value from the original input image. Here, nearest-neighbour resampling logic is used.\nA quadratic polynomial can also be used for transformations, but this is usually only used when there are serious geometric errors, usually in imagery obtained from suborbital aerial platforms.\n\\[\nx' = c_0 + c_1x + c_2y + c_3xy + c_4x^2 + c_5y^2\n\\]\n\\[\ny' = d_0 + d_1x + d_2y + d_3xy + d_4x^2 + d_5y^2\n\\]\nComputing the Root-Means-Squared-Error of the Inverse Mapping Function\nUsing RMSE allows us to determine how well the 6 coefficients derived from the least-squares regression of the initial GCPs account for the geometric distortion in the input image.\nThe user specifies a threshold of acceptable total RSME, and the GCP with the most individual error will be deleted, then recompute the 6 coefficients and RMSE for all points, until the RMSE is less than the threshold or there are too few GCPs remaining for a regression.\n\n\nIntensity Interpolation: Pixel brightness values should be determined. When a pixel in the rectified output image requires a value from the input pixel that does not fall neatly on a row-and-column coordinate, there must be a mechanism for determining the Brightness Value (BV) to be assigned to the output rectified pixel.\n\nSeveral methods of Brightness Value interpolation including nearest neighbour, bilinear interpolation and cubic convolution\nNearest neighbour is computationally efficient and does not alter BVs during resampling. It should be used when biophysical information is to be extracted from the dataset.\nBilinear interpolation assigns pixel values by interpolating BVs in 2 orthogonal directions, computing a new BV based on weighted distances to the nearest 4 pixel values. This method acts as a spatial moving filter that subdues extremes in BVs in the output image.\nCubic convolution assigns values similarly to bilinear interpolation, except that weighted values of 16 input pixels are used.\n\n\nImage-to-image registration\nThis is the translation and rotation alignment process by which 2 images of similar geometry and of same geographic area are positioned coincident to each other so that corresponding elements appear in the same place. This is used when it is not necessary to have each pixel assigned a unique x,y coordinate. A hybrid approach that uses both image-to-map rectification and image-to-image registration might be useful when detecting change between 2 or more dates of remotely sensed data."
  },
  {
    "objectID": "week3.html#summary-of-practical",
    "href": "week3.html#summary-of-practical",
    "title": "3  Week 3: Corrections",
    "section": "3.3 Summary of Practical",
    "text": "3.3 Summary of Practical\nThis practical covered basic correction concepts such as atmospheric correction, mosaicking and enhancements. For this practical, I decided to look at Landsat8 imagery of Serengeti National Park (SNP) in Tanzania. I will be summarising my takeaways from the mosaicking and ratio enhancement sections of the practical, as I found them to be particularly useful. I was also unable to perform the PCA section as it took too long for my computer.\n\n3.3.1 Mosaicking\nFor SNP, 3 tiles were needed to have a fuller view of the whole national park, so I downloaded 3 tiles that covered most of it. The first image shows the extent covered by the downloaded tiles, while the second shows the mosaicked output from R.\n\n\n\n\n\nFig. 1: Extent of tiles\n\n\n\n\n\n\n\n\n\nFig. 2: Mosaicked output\n\n\n\n\nThe mosaicked output in R was written to a GeoTiff file and then viewed in QGIS. It appears slightly off but I am unsure what went wrong in the process or this is an expected outcome.\n\n\n3.3.2 Ratio enhancements: NDVI\nThe Normalised Difference Vegetation Index is an application of ratioing, based on the fact that green vegetation absorbs the Red wavelength but reflects more in the NIR wavelength. This is illustrated as:\n\n\n\n\n\nFig. 3: Spectral traits of vegetation\n\n\n\n\nSource: PhysicsOpenLab\n\\[\nNDVI =  \\frac{NIR - Red}{NIR + Red}\n\\]\nApplying the NDVI formula to my dataset, I obtained first\n\n\n\n\n\nFig. 4: NDVI of Serengeti National Park\n\n\n\n\nWe can also focus on areas where NDVI is equal or greater than 0.1:\n\n\n\n\n\nFig. 5: NDVI of Serengeti National Park 2\n\n\n\n\nWe observe that the vegetation is not as healthy as we might expect, and this is because the images used were between 24/10/22 and 31/12/22 which coincides with the drier period.\nThis also coincides with the visualisations obtained when using the Normalised Difference Moisture Index (NDMI) and focusing on areas where NDMI is equal or greater than 0.1:\n\n\n\n\n\nFig. 6: NDMI of Serengeti National Park\n\n\n\n\n\n\n\n\n\nFig. 7: NDMI of Serengeti National Park2\n\n\n\n\nData fusion is the process of appending new raster data to existing datasets or creating new raster datasets with different bands, and data fusion can be done with newly-created texture measures and the original data. Fusion can be done in R using the stack() function.\nWhen merging datasets obtained from different remote sensors, all datasets should be accurately registered to one another and resampled to the same pixel size. One component-substitution method available is Principle Component Analysis (PCA). At this stage I would have done Principal Component Analysis to scale the raster datasets, but unfortunately I was unable to complete the PCA part of the practical. PCA allows comparison of data that is not easily comparable in its raw form. Performing PCA would transform the original data to produce uncorrelated principal component images (Pratt, 2013). An advantage of PCA-based pan-sharpening is that the number of bands is not restricted (Klonus and Ehlers, 2009)"
  },
  {
    "objectID": "week3.html#applications-of-ratio-enhancements",
    "href": "week3.html#applications-of-ratio-enhancements",
    "title": "3  Week 3: Corrections",
    "section": "3.4 Applications of Ratio Enhancements",
    "text": "3.4 Applications of Ratio Enhancements\nThe normalised difference vegetation index (NDVI) is widely used for vegetation studies, with the NASA Global Inventory, Monitoring and Modelling Studies (GIMMS) global coverage dataset (Tucker et al., 2005) being the most widely used AVHRR (Advanced Very High Resolution Radiometer) dataset. They form a relatively robust basis for detecting long-term trends in NDVI in most of the world’s semi-arid, dry sub-humid and sub-humid areas (Fensholt and Proud, 2012).\nNDVI values can be used to determine and analyse drought-prone areas, such as Faridatul and Ahmed’s (2020) work incorporating NDVI values into a modified vegetation condition index (mVCI) which enhances the detection of agricultural drought in the study area of Bangladesh; or can be used to analyse crop yields based on Pinto et al.’s (2017) work on canola yields in Brazil.\nFocusing on Serengeti National Park, I thought it would be useful to use NDVI on satellite imagery of SNP for potential applications of monitoring anthropogenic impacts on the Protected Area or impacts of climate change on the ecosystem. For example, Boone et al (2006) use rainfall and vegetation data (and NDVI) to model Serengeti wildebeest migratory patterns. However, Anderson et al (anderson?) also make the point that we should be cautious when using NDVI to study wildlife hotspots as NDVI can represent the effects of multiple, correlated processes (e.g. biomass, forage quality, cover for predators etc.) that influence the presence of hotspots (e.g. herbivore hotspots)."
  },
  {
    "objectID": "week3.html#reflections",
    "href": "week3.html#reflections",
    "title": "3  Week 3: Corrections",
    "section": "3.5 Reflections",
    "text": "3.5 Reflections\nAs Andy mentioned in the lectures, it is unlikely that we would have to perform geometric corrections given that most datasets are “Analysis Ready Data” (ARD), but it is useful to know in the event that we encounter data that requires geometric correction, or even just understanding how the data products we use have been treated from their raw form.\nMosaicking is a useful process given the limitations of some remote sensing products regarding geographical extents, and might come in handy in future research. Nonetheless, Andy also mentioned how this process may not be as important today as it is incorporated in the Google Earth Engine (GEE) workflow, so I am excited to learn in future weeks how this process is done in GEE. Mosaicking is also a bit of a throwback personally, back to 2017-18 for me when I learnt how to orthorectify and mosaic in ERDAS Imagine software (which I realised I have forgotten most of by now) and I am excited to learn how the process has evolved today when we start using GEE.\n\n\n\n\nBoone, R. B., Thirgood, S. J. and Hopcraft, J. G. C. (2006) “Serengeti wildebeest migratory patterns modeled from rainfall and new vegetation growth,” Ecology, 87(8), pp. 1987–1994. Available at: https://www.jstor.org/stable/20069184.\n\n\nBuiten, H. J. and Putten, B. van (1997) “Quality assessment of remote sensing image registration  analysis and testing of control point residuals,” ISPRS Journal of Photogrammetry and Remote Sensing, 52(2), pp. 57–73. doi: 10.1016/S0924-2716(97)83001-8.\n\n\nFaridatul, M. I. and Ahmed, B. (2020) “Assessing Agricultural Vulnerability to Drought in a Heterogeneous Environment: A Remote Sensing-Based Approach,” Remote Sensing, 12(20), p. 3363. doi: 10.3390/rs12203363.\n\n\nFensholt, R. and Proud, S. R. (2012) “Evaluation of Earth Observation based global long term vegetation trends  Comparing GIMMS and MODIS global NDVI time series,” Remote Sensing of Environment, 119, pp. 131–147. doi: 10.1016/j.rse.2011.12.015.\n\n\nJensen, J. R. (2015) Introductory Digital Image Processing: A Remote Sensing Perspective. Pearson Education, Incorporated.\n\n\nKlonus, S. and Ehlers, M. (2009) “2009 12th international conference on information fusion,” in, pp. 1409–1416.\n\n\nNovak, K. (1992) “Rectification of Digital Imagery,” PHOTOGRAMMETRIC ENGINEERING.\n\n\nPinto, D. G. et al. (2017) “Correlations between spectral and biophysical data obtained in canola canopy cultivated in the subtropical region of Brazil,” Pesquisa Agropecuária Brasileira, 52(10), pp. 825–832. doi: 10.1590/s0100-204x2017001000001.\n\n\nPratt, W. (2013) Introduction to Digital Image Processing. Boca Raton: CRC Press. Available at: https://learning.oreilly.com/library/view/introduction-to-digital/9781482216691/.\n\n\nTucker, C. J. et al. (2005) “An extended AVHRR 8-km NDVI dataset compatible with MODIS and SPOT vegetation NDVI data,” International Journal of Remote Sensing, 26(20), pp. 4485–4498. doi: 10.1080/01431160500168686.\n\n\nWolberg, G. (1990) Digital image warping | wiley. New York: John Wiley- IEEE Computer Society. Available at: https://www.wiley.com/en-us/Digital+Image+Warping-p-9780818689444."
  },
  {
    "objectID": "week3.html#references",
    "href": "week3.html#references",
    "title": "3  Week 3: Corrections",
    "section": "3.6 References",
    "text": "3.6 References\n\n\n\nAnderson, T. M., J. G. C. Hopcraft, S. Eby, M. Ritchie, J. B. Grace, H. Olff (2010) ‘Landscape-scale analyses suggest both nutrient and antipredator advantages to Serengeti herbivore hotspots’, Ecology, 91(5), 1519-29.\nBoone, R. B., S. J. Thirgood, J. G. C. Hopcraft (2006) ‘Serengeti Wildebeest Migratory Patterns Modeled from Rainfall and New Vegetation Growth’, Ecology, 87(8), 1987-94.\nBuiten, H. J. and B. Van Putten (1997) ‘Quality Assessment of Remote Sensing Registration- Analysis and Testing of Control Point Residuals’, ISPRS Journal of Photogrammetry & Remote Sensing, 52, 57-73. Available at: https://doi.org/10.1016/S0924-2716(97)83001-8\nFaridatul, M. I. and B. Ahmed (2020) ‘Assessing Agricultural Vulnerability to Drought in a Heterogeneous Environment: A Remote Sensing-Based Approach’, Remote Sensing, 12(20), 3363.\nFensholt, R. and S. R. Proud (2012) ‘Evaluation of Earth Observation based global long term vegetation trends — Comparing GIMMS and MODIS global NDVI time series’, Remote Sensing of Environment, 119, 131-47.\nJensen, J.R. (2015) Introductory Digital Image Processing, 4th edn, Pearson Higher Education US (A Remote Sensing Perspective).\nKlonus, S. and M. Ehlers (2009) ‘Performance of Evaluation Methods in Image Fusion’, Proceedings of the 12th International Conference on Information Fusion, Seattle, July 6-9 2009, p. 8.\nNovak, K. (1992) ‘Rectification of Digital Imagery’, Photogrammeric Engineering & Remote Sensing, 58(3), 339-44.\nPinto, D. G., D. C. Fontana, G. A. Damalgo, E. Fochesatto, M. B. Vicari, C. Bremm, G. R. da Cunha, J. A. de Gouvea, A. Santi (2017) ‘Correlations between spectral and biophysical data obtained in canola canopy cultivated in the subtropical region of Brazil’, Pesquisa Agropecuária Brasileira, 52 (10), 825-32.\nPratt, W. K. (2013) Introduction to Digital Image Processing, Boca Raton: CRC Press, p. 736.\nTucker, C. J., J. E. Pinzon, M. E. Brown, D. A. Slayback, E. W. Pak, R. Mahoney (2005) ‘An extended AVHRR 8-km NDVI dataset compatible with MODIS and SPOT vegetation NDVI data’, International Journal of Remote Sensing, 26, 4485-98.\nWolberg, G. (1990) Digital Image Warping, NY: John Wiley- IEEE Computer Society, p. 340.\n\n\n\n\nBoone, R. B., Thirgood, S. J. and Hopcraft, J. G. C. (2006) “Serengeti wildebeest migratory patterns modeled from rainfall and new vegetation growth,” Ecology, 87(8), pp. 1987–1994. Available at: https://www.jstor.org/stable/20069184.\n\n\nBuiten, H. J. and Putten, B. van (1997) “Quality assessment of remote sensing image registration  analysis and testing of control point residuals,” ISPRS Journal of Photogrammetry and Remote Sensing, 52(2), pp. 57–73. doi: 10.1016/S0924-2716(97)83001-8.\n\n\nFaridatul, M. I. and Ahmed, B. (2020) “Assessing Agricultural Vulnerability to Drought in a Heterogeneous Environment: A Remote Sensing-Based Approach,” Remote Sensing, 12(20), p. 3363. doi: 10.3390/rs12203363.\n\n\nFensholt, R. and Proud, S. R. (2012) “Evaluation of Earth Observation based global long term vegetation trends  Comparing GIMMS and MODIS global NDVI time series,” Remote Sensing of Environment, 119, pp. 131–147. doi: 10.1016/j.rse.2011.12.015.\n\n\nJensen, J. R. (2015) Introductory Digital Image Processing: A Remote Sensing Perspective. Pearson Education, Incorporated.\n\n\nKlonus, S. and Ehlers, M. (2009) “2009 12th international conference on information fusion,” in, pp. 1409–1416.\n\n\nNovak, K. (1992) “Rectification of Digital Imagery,” PHOTOGRAMMETRIC ENGINEERING.\n\n\nPinto, D. G. et al. (2017) “Correlations between spectral and biophysical data obtained in canola canopy cultivated in the subtropical region of Brazil,” Pesquisa Agropecuária Brasileira, 52(10), pp. 825–832. doi: 10.1590/s0100-204x2017001000001.\n\n\nPratt, W. (2013) Introduction to Digital Image Processing. Boca Raton: CRC Press. Available at: https://learning.oreilly.com/library/view/introduction-to-digital/9781482216691/.\n\n\nTucker, C. J. et al. (2005) “An extended AVHRR 8-km NDVI dataset compatible with MODIS and SPOT vegetation NDVI data,” International Journal of Remote Sensing, 26(20), pp. 4485–4498. doi: 10.1080/01431160500168686.\n\n\nWolberg, G. (1990) Digital image warping | wiley. New York: John Wiley- IEEE Computer Society. Available at: https://www.wiley.com/en-us/Digital+Image+Warping-p-9780818689444."
  },
  {
    "objectID": "week1.html#refs",
    "href": "week1.html#refs",
    "title": "1  Week 1: Introduction to Remote Sensing",
    "section": "1.6 References",
    "text": "1.6 References\n\n\n\n\nCorcione, Valeria, Andrea Buono, Ferdinando Nunziata, and Maurizio Migliaccio. 2021. “A Sensitivity Analysis on the Spectral Signatures of Low-Backscattering Sea Areas in Sentinel-1 SAR Images.” Remote Sensing 13 (6, 6): 1183. https://doi.org/10.3390/rs13061183."
  },
  {
    "objectID": "week1.html#section",
    "href": "week1.html#section",
    "title": "2  Week 1: Introduction to Remote Sensing",
    "section": "2.6 ",
    "text": "2.6 \n\n\n\n\nCorcione, V. et al. (2021) “A Sensitivity Analysis on the Spectral Signatures of Low-Backscattering Sea Areas in Sentinel-1 SAR Images,” Remote Sensing, 13(6), p. 1183. doi: 10.3390/rs13061183."
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "5  Week 4: Policy",
    "section": "",
    "text": "Singapore is a tropical city-state located in Southeast Asia, and is well-known for being a “Green City” for its extensive incorporation of greenery into its urban form. In light of climate change and the need to transit to more sustainable forms of development, the Singapore government launched the Singapore Green Plan 2030in 2021 to drive the national agenda on sustainable development. As a low-lying island state, climate change is an existential threat for Singapore and there is a need to ensure its resilience to climate threats. There are 5 key pillars to the Singapore Green Plan (SGP):\n\nCity in Nature\n\nKey Goal: Set aside 50% more land (200 ha) for nature parks, intensify nature in gardens and parks, and for every household to live within 10 minutes walk of a park\n\nEnergy Reset\n\nKey Goals: Quadruple solar energy deployment by 2025, including covering rooftops of state-subsidised housing blocks with solar panels, and reduce domestic greenhouse gas emissions by at least 3 million tonnes per year by 2030.\n\nSustainable Living\n\nKey Goals: Reduce waste sent to landfills by 30% and encourage walking and cycling as transport options.\n\nGreen Economy\n\nKey Goals: Promote Green Finance and carbon trading\n\nResilient Future\n\nKey Goals: Better understand and protect coastlines against rising sea levels, and limit the urban heat island effect.\n\n\nFor the policy goals of protecting coastlines and limiting urban heat effects, it is admirable that Singapore is looking to incorporate remotely sensed data into its workflow for achieving these goals. To better understand and model rising sea levels and their effects on Singapore, the Public Utilities Board (PUB) has collaborated with National University of Singapore (NUS) to use remotely-sensed data and geospatial models. As for efforts mitigating Urban Heat Island effect, the National Parks agency (NParks) has deployed an island-wide network of climate sensors that collect data on wind speeds, humidity and temperature. The collected data will then be used in the Singapore Variable Resolution (SINGV) model (which models future climatic scenarios) and the Integrated Environment Modeller (IEM) (which helps urban planners optimise building layouts).\nRemote sensing data can be further incorporated into the the Singapore Green Plan workflow, and I would propose using it to help in the achievement of the City in Nature goal. It should also be noted that the City in Nature goal is in line with Sustainable Development Goals (SDGs) 11 (Sustainable Cities and Communities), 13 (Climate Action) and 15 (Life on Land)."
  },
  {
    "objectID": "week4.html#application-of-remotely-sensed-data-to-the-city-in-nature-goal",
    "href": "week4.html#application-of-remotely-sensed-data-to-the-city-in-nature-goal",
    "title": "5  Week 4: Policy",
    "section": "5.2 Application of remotely sensed data to the City in Nature goal",
    "text": "5.2 Application of remotely sensed data to the City in Nature goal\nI would propose the use of remote sensing methods to measure and track the intensity of nature in gardens and parks.\nBefore I go into the details of how this can be done specifically in Singapore’s context, I will first do a literature review on current methodologies on urban vegetation.\nFor this, I refer to Neyns and Canters’ (2022) overview on current literature on urban vegetation in remote sensing. Scholars either define vegetation types based on functionality or taxonomic classes. Studies that analyse vegetation type by functionality tend to focus on the provision of ecosystem services., while classification by species in the urban environment tends to be more challenging due to noise.\nAs for the type of sensor data, it is noted that high spatial resolution (which is desirable to ensure that the vegetation is larger than a pixel) usually comes with a tradeoff on spectral resolution (which is better for mapping results). As for spectral bands, Li et al (2015)] found that the newly added red edge and NIR2 bands of Worldview 2 and 3 contribute more to the differentiation of tree species compared to the traditional four bands of Worldview 1 (red, green, blue, NIR). The mapping of individual trees becomes easier from a resolution of 3m or higher, and both spaceborne and airborne sensors can produce imagery at this resolution. Degerickx et al (2020) also found that structural variables derived from LiDAR data was more useful than hyperspectral variables. Using multi-temporal data is also useful to assess the influence of the time of data acquisition as well as in the classification process. However, given the relative stability of Singapore’s weather throughout the year, this may not be as essential.\nAs for feature definition, there are several types discussed such as spectral, textural, geometric, contextual and LIDAR-derived features. Spectral features use vegetation indices such as NDVI (which was covered in week 3), although NDVI in urban environments may lead to the possible false labelling of red clay roofs as vegetation (Zhang, Feng and Jiang, 2010). Geometric features describing the size, shape and edge complexity of objects can be useful in identifying broader functional vegetation types due to their widely different spatial properties (i.e. the space they occupy). Contextual features use neighbouring characteristics, and can be used for the mapping of functional vegetation types, such as Wen et al. (2017) differentiating between road-side, park and residential trees by considering the relation between trees in a predefined area.\nWang et al (2019) concluded that fusion of spectral imagery with Light Detection And Ranging (LiDAR) data substantially improves the identification of tree species in a urban setting. Authors also deal with shadows in various ways such as omitting elements affected by shadow, or by performing shadow correction, or by including shadowed areas as separate classes.\nReferring to Ren et al’s (2017) work, we see that using NDVI with Landsat TM data is also useful in rapdily estimating urban vegetation structural attributes such as leaf area index (LAI), crown closure (CC) and basal area (BA) at a spatiotemporal 30m resolution. NDVI applied to Sentinel-2 images, combined with plant height information (using Digital Object Height Models) is also another method of analysing he spatial distribution of well-equipped greenspace areas with high health and recreational potential as well as areas for improvement in poorly-equipped urban areas (Juergens and Meyer-Heß, 2022).\nCircling back to how Singapore can use remote sensing methods to plan and monitor greenspaces and vegetation intensity, I would propose using Landsat or Sentinel imagery with NDVI, combined with 3D datasets to analyse greenspaces and plan where improvements in greenspaces should occur based on Juergens and Meyer-Heß’s (2022)methodology. Following which, I propose that remote sensing methods also be incorporated into a monitoring process, to measure the change in vegetation intensity and if planned improvements to urban vegetation are successful. Given that this measurement is not needed frequently (probably once a year), I would recommend that airborne sensors be used, so that data of high spatial and spectral resolution can be gathered, making the measurement of urban vegetation intensity more accurate. LiDAR data should also be included so that it can be combined with spectral imagery. This will allow Singapore to more easily quantify and measure the effectiveness of their efforts to intensify urban vegetation in its goal to become a City in Nature."
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "5  Week 4: Policy",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nThinking about policy applications was helpful as it pushed me to consider the specific data sources and methodologies that could be used to achieve particular goals, and this was helpful as we could think through specific details rather than just “know” that remotely sensed data could be applied in different many ways. Obviously, there are more ways we can think about the practical applications of remotely sensed data in policy, but I enjoyed this start. While I now roughly understand the steps that can be taken to apply remote sensing methodologies in a policy context, I think the next step is to understand how I can actually do it practically, which I look forward to learning more about in the rest of the module.\n\n\n\n\nDegerickx, J., Hermy, M. and Somers, B. (2020) “Mapping Functional Urban Green Types Using High Resolution Remote Sensing Data,” Sustainability, 12(5), p. 2144. doi: 10.3390/su12052144.\n\n\nJuergens, C. and Meyer-Heß, M. F. (2022) “Experimental Analysis of Geo-spatial Data to Evaluate Urban Greenspace: A Case Study in Dortmund, Germany,” KN - Journal of Cartography and Geographic Information, 72(2), pp. 153–171. doi: 10.1007/s42489-022-00107-5.\n\n\nLi, D. et al. (2015) “Object-Based Urban Tree Species Classification Using Bi-Temporal WorldView-2 and WorldView-3 Images,” Remote Sensing, 7(12), pp. 16917–16937. doi: 10.3390/rs71215861.\n\n\nNeyns, R. and Canters, F. (2022) “Mapping of Urban Vegetation with High-Resolution Remote Sensing: A Review,” Remote Sensing, 14(4), p. 1031. doi: 10.3390/rs14041031.\n\n\nRen, Z. et al. (2017) “Spatiotemporal analyses of urban vegetation structural attributes using multitemporal Landsat TM data and field measurements,” Annals of Forest Science, 74(3), pp. 1–14. doi: 10.1007/s13595-017-0654-x.\n\n\nWang, K., Wang, T. and Liu, X. (2019) “A Review: Individual Tree Species Classification Using Integrated Airborne LiDAR and Optical Imagery with a Focus on the Urban Environment,” Forests, 10(1), p. 1. doi: 10.3390/f10010001.\n\n\nWen, D. et al. (2017) “Semantic classification of urban trees using very high resolution satellite imagery,” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 10(4), pp. 1413–1424. doi: 10.1109/JSTARS.2016.2645798.\n\n\nZhang, X., Feng, X. and Jiang, H. (2010) “Object-oriented method for urban vegetation mapping using IKONOS imagery,” International Journal of Remote Sensing, 31(1), pp. 177–196. doi: 10.1080/01431160902882603."
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "6  Week 5: Google Earth Engine (GEE)",
    "section": "",
    "text": "Understand how GEE works\nFamiliarise self with using GEE and its functionalities."
  },
  {
    "objectID": "week5.html#summary-of-key-content",
    "href": "week5.html#summary-of-key-content",
    "title": "6  Week 5: Google Earth Engine (GEE)",
    "section": "6.2 Summary of Key Content",
    "text": "6.2 Summary of Key Content\n\n6.2.1 Background of Google Earth Engine\nGEE is a geospatial processing service that allows geospatial analysis at scale (volume-wise). It takes code that users write and applies it to data on their servers. GEE has unique names for the types of data we use, “image” for what we know as raster data and “feature” for what we know as vectors. There is also ImageCollection and FeatureCollections, which are equivalents of raster stacks and (possibly?) geodatabases. GEE also uses Javascript, which will be interesting as I have no prior exposure to Javascript.\n\n\n6.2.2 Concept of Client vs Server side\nWe can upload our own data on GEE, and that is on client side. But the other GEE data is on server side. We refer to these Earth Engine Objects using “ee” in front of it, and they are not available locally.\nThis implies that we shouldn’t loop on the server. Instead we create a function, and then apply it to what we want on the server. However, there are functions available on the server side too, and they are referred to with the “ee” in front too. Mapping is useful to be more computationally efficient when using GEE\n\n\n6.2.3 Scale/ resolution\nImage scale in GEE refers to pixel resolution. When analysing in GEE, GEE aggregates the image to fit a 256x256 grid. We have to set the scale parameter we need otherwise GEE will resample using nearest neighbour. GEE is aggregating the values based on our analysis extent. We can Input the minimum and maximum for bands to control how the image is visualised as well.\n\n\n6.2.4 Projections\nGEE uses EPSG 3857 as the default projection, and we do not need to worry about it until the end when we are exporting."
  },
  {
    "objectID": "week5.html#gee-in-action-summary-of-practical-content",
    "href": "week5.html#gee-in-action-summary-of-practical-content",
    "title": "6  Week 5: Google Earth Engine (GEE)",
    "section": "6.3 GEE in Action: Summary of Practical Content",
    "text": "6.3 GEE in Action: Summary of Practical Content\nThis week’s practical covers skills that we have covered before, but in GEE such as optimising the imagery used for analysis, mosaicking and clipping images, as well as texturing, PCA and indices. For this practical, I will be using New Delhi, which is the city Andy used in the practical, as I view this practical as more of a way to familiarise myself with GEE and future GEE practicals will be attempted with other cities.\n\n6.3.1 GEE Data\nOne thing that stood out to me was the wide variety of datasets available in Google Earth Engine’s Data Catalog. There were datasets available for Climate and Weather data (Surface Temperatures, Climate models, Atmospheric Data, Weather), Imagery (Landsat, Sentinel), MODIS and High-Resolution Imagery) as well as Geophysical data (DEMs, Land Cover maps, Cropland and other datasets such as night-time imagery). This wide variety makes GEE very useful for multiple types of methodologies and analysis, and I would be interested in exploring all the other datasets if time permits.\n\n\n6.3.2 Scaling factors\nScaling factors from Landsat surface reflectance product is something we have not really encountered before, and it is helpful to know this for future processing of Landsat imagery. However, I have not been able to find a good explanation online on the need to scale the imagery. After using scaling the imagery, I obtained a similar image to Andy’s.\n\n\n\n\n\nFig. 1: True Colour\n\n\n\n\nAfter scaling, mosaicking (including obtaining the mean of overlapping pixels) and clipping was done to obtain a smoother image below that fit New Delhi’s boundaries.\n\n\n\n\n\nFig. 2: Mosaicked and Clipped\n\n\n\n\n\n\n6.3.3 Texturing\nSatellite image texture quantifies spectral and spatial variations in pixel values of an image (Farwell et al., 2021), therefore conveying information about spectral and spatial heterogeneity of image features (Haralick, Shanmugam and Dinstein, 1973). The specific statistical method we apply here is the gray-level co-occurrence matrix (GLCM) which is a tabulation of how often different combinations of pixel brightness values (gray levels) occur in an image (Hall-Beyer, 2017). It characterises the texture of an image by calculating how often pairs of pixels with specific values and in a specified spatial relationship occur in an image, thus creating a GLCM, and then statistical measures are extracted from this matrix. Texture measures then give us concepts like “contrast”, “dissimilarity” and “entropy”.\nWe compute GLCM texture for our area of interest and obtain the following output:\n\n\n\n\n\nFig. 3: GLCM Texture of New Delhi\n\n\n\n\nWe then zoom in to a specific area to observe what the computed GLCM texture tells us:\n\n\n\n\n\n\n\n(a) True Colour\n\n\n\n\n\n\n\n(b) GLCM\n\n\n\n\nFigure 6.1: Comparison of True Colour Image and GLCM Texture to understand what it shows\n\n\nWe see that the purple spots in the GLCM texture appear to be buildings that are especially reflective in the True Colour image.\n\n\n6.3.4 PCA\nWhen performing PCA, we are transforming the multi-spectral data we have into a uncorrelated and smaller dataset, while keeping most of the original information. The first component should also capture most of the variance within the dataset. This was the PCA output obtained:\n\n\n\n\n\nFig. 4: PCA\n\n\n\n\n\n\n\n\n\n\n\n(a) True Colour\n\n\n\n\n\n\n\n(b) PCA\n\n\n\n\nFigure 6.2: Comparison of True Colour Image and PCA output to understand what it shows"
  },
  {
    "objectID": "week5.html#applications-of-key-concepts",
    "href": "week5.html#applications-of-key-concepts",
    "title": "6  Week 5: Google Earth Engine (GEE)",
    "section": "6.4 Applications of key concepts",
    "text": "6.4 Applications of key concepts\nGLCM and PCA can be used together in remote sensing research. In forestry mapping, such as mapping bamboo forests, textural features such as homogeneity, contrast, entropy and variance of GLCM can be used as classification features(Qi et al., 2022). In this study, textural features were derived from the first component image from a PCA. In a object-oriented LULC classification study (Tassi and Vizzari, 2020), they used GLCM to first calculate textural indices from satellite imagery, and then doing a PCA of the most relevant GLCM metrics to synthesize the textural information. Based on what we observe from current literature, GLCM is usually used to extract textural information from the raster datasets, and PCA done to reduce the number of bands and extract key components, making further steps such as classification easier(Shafizadeh-Moghadam et al., 2021)."
  },
  {
    "objectID": "week5.html#reflections",
    "href": "week5.html#reflections",
    "title": "6  Week 5: Google Earth Engine (GEE)",
    "section": "6.5 Reflections",
    "text": "6.5 Reflections\nThis week’s content has familiarised myself with GEE, and hopefully with further use of it, I will become better at understanding the breadth of functions it has in remote sensing analysis. I will also become more aware of limitations of GEE, such as how it might not be the best for doing in-depth geographic analysis with raster data as it has limited analytical functions compared to R for example. We saw how to do classic remote sensing processing steps such as mosaicking and clipping in GEE.\nThe practical on texturing was useful in helping me understand how raster data works and how it can be used practically for landcover classification. This knowledge seems to be useful for what we’ll be covering in future weeks.\n\n\n\n\nFarwell, L. S. et al. (2021) “Satellite image texture captures vegetation heterogeneity and explains patterns of bird richness,” Remote Sensing of Environment, 253, p. 112175. doi: 10.1016/j.rse.2020.112175.\n\n\nHall-Beyer, M. (2017) “GLCM Texture: A Tutorial v. 3.0 March 2017.”\n\n\nHaralick, R. M., Shanmugam, K. and Dinstein, I. (1973) “Textural features for image classification,” IEEE Transactions on Systems, Man, and Cybernetics, SMC-3(6), pp. 610–621. doi: 10.1109/TSMC.1973.4309314.\n\n\nQi, S. et al. (2022) “Bamboo Forest Mapping in China Using the Dense Landsat 8 Image Archive and Google Earth Engine,” Remote Sensing, 14(3), p. 762. doi: 10.3390/rs14030762.\n\n\nShafizadeh-Moghadam, H. et al. (2021) “Google earth engine for large-scale land use and land cover mapping: An object-based classification approach using spectral, textural and topographical factors,” GIScience & Remote Sensing, 58(6), pp. 914–928. doi: 10.1080/15481603.2021.1947623.\n\n\nTassi, A. and Vizzari, M. (2020) “Object-Oriented LULC Classification in Google Earth Engine Combining SNIC, GLCM, and Machine Learning Algorithms,” Remote Sensing, 12(22), p. 3776. doi: 10.3390/rs12223776."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "8  Week 7: Classification 2",
    "section": "",
    "text": "Object-based image analysis (OBIA)\n\nSuperpixels\nSimple Linear Iterative Clustering (SLIC) Algorithm for superpixel generation\nConsiders shapes based on similarity/ difference\nParameters: distance (closeness to centre) and homogeneity of colours\n\nSub-pixel analysis/ sub pixel classification/ spectral mixture analysis/ linear spectral unmixing\n\nestimate fractions that make up a pixel\nfew endmembers that are spectrally pure\n\nSpectral library\nFrom image\nlabwork\nSub pixel analysis matrices: GEE makes it unconstrained by default, but constrain by setting sum to 1\n\nMakes a map of fractions\nNumber of endmembers: Vegetation–Impervious Surface- Soil model\nMultiple endmember spectral analysis (MESMA)\n\n\n\n\n\nPA Producer accuracy\nUA User Accuracy\nOA Overall Accuracy\nConfusion matrix\nFuzzy matrix\nKappa\n\n\n\n\nCombine into F1 score (Both PA and UA)\nF1 doesnt consider true negatives\nbut depends on threshold\nReceiver Operating Characteristic Curve\nCalc area under curve\nCross-validation\nSpatial auto-correlation\nso use spatial cross-validation ideally\n\n\n\n\nSpatially partition the folded data\nDistance threshold (GEE)\nDisjoint (no common boundary) using k-means clustering\nSupport Vector Machine\n\nLovelace: rather than using each fold to figure out C and gamma (overfits), take random sample from fold then get C and gamma.\nNot available in GEE yet but available in R\nGEE specific to satellite imagery, hard to do vector analysis and stuff"
  },
  {
    "objectID": "week7.html#application",
    "href": "week7.html#application",
    "title": "8  Week 7: Classification 2",
    "section": "8.2 Application",
    "text": "8.2 Application\nHowever, one challenge for landcover classification is that of urban areas. Identifying and delineating urban areas is still a challenge because of the difficulty of disentangling surface reflectance of pixels due to the varied types of surface materials and structures in urban areas (Herold, Gardner and Roberts, 2003; Varshney and Rajesh, 2014; MacLachlan et al., 2017). Spatial resolution is thus an important factor for accurate monitoring of LULC change.\nAccurate estimation of urban areas is also important as it is also used in other applications such as climate models or investigating urban heat island effects. What I found interesting in Andy’s article (2017) is how remote sensing methods are still developing and improving. In his article, he compares the use of Support Vector Machine (SVM) spectral unmixing and the novel sub and hard pixel Import Vector Machine (IVM) classifier. It also covers VIS (week 7)\nchallenges in urban areas (Zhou et al 2009)"
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "8  Week 7: Classification 2",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\n\n\n\n\nHerold, M., Gardner, M. E. and Roberts, D. A. (2003) “Spectral resolution requirements for mapping urban areas,” IEEE Transactions on Geoscience and Remote Sensing, 41(9), pp. 1907–1919. doi: 10.1109/TGRS.2003.815238.\n\n\nMacLachlan, A. et al. (2017) “Subpixel land-cover classification for improved urban area estimates using Landsat,” International Journal of Remote Sensing, 38(20), pp. 5763–5792. doi: 10.1080/01431161.2017.1346403.\n\n\nVarshney, A. and Rajesh, E. (2014) “A Comparative Study of Built-up Index Approaches for Automated Extraction of Built-up Regions From Remote Sensing Data,” Journal of the Indian Society of Remote Sensing, 42(3), pp. 659–663. doi: 10.1007/s12524-013-0333-9."
  },
  {
    "objectID": "week6.html#summary-of-key-concepts",
    "href": "week6.html#summary-of-key-concepts",
    "title": "7  Week 6: Classification 1",
    "section": "7.2 Summary of Key Concepts",
    "text": "7.2 Summary of Key Concepts\nIn this week’s lecture we looked at examples of land-use/ landcover (LULC) that we had encountered before throughout this module, before diving into classification methodologies.\nApplications of LULC classifications include urban expansion (MacLachlan et al 2017), air pollution (Fuldalu and Alta 2021), urban green spaces (Shahtahmassebi et al 2021), forest monitoring (Hansen et al) and forest fires (Chuvieco and Congalton 1989). We then looked at classification methods which this learning diary entry will focus on.\n\n7.2.1 Classification Methodologies: Classification and Regression Trees (CART)\nClassification Trees: Classifying data into 2 or more discrete categories\nWhat variable do we start a classification tree with (the root of the tree)? We use the Gini Impurity to determine that. The variable with the lowest impurity goes at the root, and use the Gini Impurity at each branch to split the nodes/ leaves.\n\\[ Gini Impurity = 1 - (probability-of-yes)^2 - (probability-of-no)^2 \\]\nTake weighted average for the variable too, and lowest impurity wins.\nWe use classification trees for landcover classification\nRegression Trees: Predicting continuous dependent variables.\nWe use Sum of Squared Residuals to divide the data into sections.\nLowest SSR wins and is used as the root of the tree. Repeat process of finding SSR for each segment to continue the classification. Each leaf is a numeric value. We can do this with many predictor variables\nOverfitting occurs when we have a leaf with just one value (pure output). We want a good balance between bias and variance.\nTo prevent overfitting, we limit how trees grow by setting a minimum number of observations in a leaf (from the top); (e.g. 20 pixels so that there is some degree of generalisation)\nor we can do weakest link pruning (with tree score). WLP prunes from the bottom. Tree score = SSR + tree penalty (alpha) * T (number of leaves).\nWeakest Link Pruning process: Run a full tree with all data. Look at leaves’ SSR. Remove a leaf, and see if SSR improves. To do that we use Tree Score. Key thing is tree penalty. We compute tree penalty by using a full size regression tree with all the data. Start with a value of 0 (which gives lowest tree score). Save values of alpha that gives a lower tree score than when a=0. Increase value of alpha until we get a lower tree score than the original.\nTrain-test split (70-30). Use training data and use alpha values from before. Calculate SSR. Calculate alpha by letting it run.\nUsing test data, and tree, calculate sum of squared residuals (for all values of alpha) –> use the value of alpha which gives lowest tree score.\nCross-validate by changing the data in test-train sets. Do it 10 times (ten-fold cross validation). Use the value of alpha that gives lowest SSR across all cross-validation\nThis process makes the tree more generalisable, and is basically identifying the weakest leaves and removing them from the tree\nDifferent classifiers give different results, but we can’t really say one classifier is better than another\n\n\n7.2.2 Comparison of classifiers\nDecision tree deals with collinearity\nIn a DT, each division could be based on different bands from input. While in SVM, its a multi-dimensional array plot and fitting planes\nRF and DT have low computation cost and easy to visualise\nSVM don’t deal with collinearity because you’re fitting a plane ((using x,y values) rather than dividng data\nUses a lot of computer memory"
  },
  {
    "objectID": "week6.html#summary-of-practical-content",
    "href": "week6.html#summary-of-practical-content",
    "title": "7  Week 6: Classification 1",
    "section": "7.3 Summary of Practical Content",
    "text": "7.3 Summary of Practical Content\n\n7.3.0.1 Overview:\nLoad admin boundary vector data\nLoad raster data by specifying image collection, date range, intersecting region of interest, and cloud percentage threshold.\nWe can either use median value of all pixels through the image stack (lazy way that neglects temporal variation in data) or take deciles (Hansen’s way) –> pattern vector becomes much bigger to draw upon the seasonality of the data.\nAdd polygons for landcovers we are interested in classifying or use a pixel approach to get more accurate results\nInsert a train-test split here to test the model\nSet the bands we are using for classification and classification property.\nTrain classifier, classify image, and plot output.\nNote the trap of spatial autocorrelation when selecting POIs and pixels to train classifiers\n\n\n7.3.0.2 Practical Output\nFor this practical I used Ulanbaatar, the capital of Mongolia. However, due to the level 2 Administrative Boundaries and to obtain different landcovers, the output covers the southern outskirts of Ulanbaatar.\nI first obtained an image stack that contained median values of all pixels over the date range (rather than percentiles).\n\n\n\n\n\n\nTrue Colour of median values for Ulanbaatar\n\n\n\n\n\n\n\nGoogle Maps’ classification of features\n\n\n\n\n\n\nUlanbaatar\n\n\n\nWe see that the river appears more like a line in the satellite imagery while it is drawn as a polygon in Google Maps. We also note the demarcation of “forest” and “mountainous” areas on Google Maps and how it appears on the satellite imagery.\n\n\n\n\n\n\nClassified Landcover\n\n\n\n\n\n\n\nClassified Pixel Landcover\n\n\n\n\n\n\nComparing classification methods\n\n\n\nThe first method trained the model using the polygons extracted, while the polygons I drew were not very precise, hence you can see polygons in the first image such as in the river and urban areas. The second method selected some pixels from each landcover class I drew, generated a train-test split, and extracted values to train the model. This pixel approach resulted in a smoother classification compared to the first one.\n\n\n\n\n\n\nLarge Classified Pixel Landcover\n\n\n\n\n\n\n\nLarge Landcover raw version\n\n\n\n\n\n\nObserving accuracy\n\n\n\nWe see that for the large administrative area with multiple landcovers, the pixel method is fairly accurate, and the overall accuracy was 78.84%."
  },
  {
    "objectID": "week6.html#application-of-key-concepts-and-skills",
    "href": "week6.html#application-of-key-concepts-and-skills",
    "title": "7  Week 6: Classification 1",
    "section": "7.4 Application of Key Concepts and Skills",
    "text": "7.4 Application of Key Concepts and Skills\nWith the accessibility of Google Earth Engine, there is an increasing number of landcover classification studies being done, some of which I presented in last week’s learning diary entry.\nLULC Classification Methods (Phiri and Morgenroth 2017)\nUrban slum detection (Kohli Sliuza Stein 2016)"
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "7  Week 6: Classification 1",
    "section": "7.5 Reflection",
    "text": "7.5 Reflection"
  },
  {
    "objectID": "week8.html",
    "href": "week8.html",
    "title": "9  Week 8: Temperature and Policy",
    "section": "",
    "text": "Urban areas are of higher temperatures\n2 main factors:\n\nMore dark surfaces that retain heat\nLess vegetation cooling the environment\n\nOther factors:\n\nLow Sky View Factor\nAir speed, cloud cover, cyclic solar radiation, building material and athropogenic energy\n\nCost of Urban Heat:\n\nSocial\nEnvironmental\n\nPositive feedback loop\n\nEconomic\n\nOnly Melbourne has assessed the cost of Urban Heat Island effect before\nUHI not really included in climate prediction models (Estrada et al 2017)\n\n\nGlobal Policy Documents/ Goals related to UHI\n\nNew Urban Agenda\nSustainable Development Goals (SDGs)\nCOP26\n\n\n\n\nSuperblocks\nMedellin Green Corridors\nTurn Down the Heat Strategy and Action Plan\nChicago 1995 Heatwave\n\n\n\nLondon Plan\nSingapore Master Plan"
  },
  {
    "objectID": "week8.html#lecture-and-practical-summary-extracting-temperature-from-satellite-data",
    "href": "week8.html#lecture-and-practical-summary-extracting-temperature-from-satellite-data",
    "title": "9  Week 8: Temperature and Policy",
    "section": "9.2 Lecture and Practical Summary: Extracting Temperature from Satellite Data",
    "text": "9.2 Lecture and Practical Summary: Extracting Temperature from Satellite Data\n\n9.2.1 Methodology to extract temperature per spatial unit\n\nSpatial data\nTemperature datasets (MODIS, Landsat etc)\n\ncollection\nfilter for standard stuff\n\nZonal statistics\noutput shapefile\n\n\n\n9.2.2 Mean Radiant Temperature"
  },
  {
    "objectID": "week8.html#applications",
    "href": "week8.html#applications",
    "title": "9  Week 8: Temperature and Policy",
    "section": "9.3 Applications",
    "text": "9.3 Applications"
  },
  {
    "objectID": "week8.html#reflections",
    "href": "week8.html#reflections",
    "title": "9  Week 8: Temperature and Policy",
    "section": "9.4 Reflections",
    "text": "9.4 Reflections\nHow achievable are these policy suggestions at various scales?"
  }
]